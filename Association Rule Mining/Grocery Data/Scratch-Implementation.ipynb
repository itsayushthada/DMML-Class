{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "with open(\"chainstoreFIM.txt\", \"r\") as file:\n",
    "    for x in file.readlines():\n",
    "        temp = x.replace(\"\\n\", \"\").split(\" \")\n",
    "        samples.append(set(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46086"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set.union(*samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little Change from Gene Dataset. This data is massive as compared to Gene Data. So we have to limit max size of itemset before even starting the algorithm. This code is written explictly for non distirbuted environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveApriori:\n",
    "    def __init__(self, data, min_support, min_confidence, max_length=None):\n",
    "        self.data = data\n",
    "        self.chars = set.union(*data)\n",
    "        self.counter = {0: {}}\n",
    "        self.rules = set()\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Thresholds\n",
    "        self.min_support = min_support\n",
    "        self.min_confidence = min_confidence\n",
    "        \n",
    "        # Metrics\n",
    "        self.get_confidence = lambda x, y: self.get_support(set.union(x, y))/self.get_support(x)\n",
    "        self.get_lift = lambda x, y: self.get_confidence(x, y)/self.get_support(y)\n",
    "    \n",
    "        # Utility\n",
    "        self.__get_key = lambda x: \"-\".join(sorted(x))\n",
    "        self.__get_set = lambda x: set(x.split(\"-\"))\n",
    "        \n",
    "        \n",
    "    def __dict_merger(self):\n",
    "        temp = {}\n",
    "        for x in self.counter.values():\n",
    "            temp = {**temp, **x}\n",
    "        self.counter = temp\n",
    "        \n",
    "    def __build_counter(self, ngram):\n",
    "        # Build counter for item length small than the given item length, if not exist.\n",
    "        if (ngram-1) not in self.counter.keys():\n",
    "            self.__build_counter(ngram-1)\n",
    "\n",
    "        # Prevent creating counter for longer itemlist is support is already exhausted for smaller counter list.\n",
    "        if (ngram-1) in self.counter:\n",
    "            if len(self.counter[ngram-1]) > 0 or (ngram-1) == 0:\n",
    "                itemset = None\n",
    "                if ngram == 1:\n",
    "                    # Generate all combinations of items.\n",
    "                    itemset = list(combinations(self.chars, ngram))\n",
    "                else:\n",
    "                    # Apriori property.\n",
    "                    itemset = list(combinations(set(\"-\".join((self.counter[ngram-1]).keys()).split(\"-\")), ngram))\n",
    "\n",
    "                # Count for itemlist of length ngram.\n",
    "                item_count = {self.__get_key(x): 0 for x in itemset}\n",
    "                for x in itemset:\n",
    "                    for y in self.data:\n",
    "                        x = set(x)\n",
    "                        if len(set.intersection(x, y)) >= ngram :\n",
    "                            item_count[self.__get_key(x)] += 1\n",
    "                \n",
    "                # Drop item combiantion with lower support than threshold.\n",
    "                keys = list(item_count.keys())\n",
    "                for key in keys:\n",
    "                    if item_count[key] < self.min_support:\n",
    "                        item_count.pop(key)\n",
    "\n",
    "                self.counter[ngram] = item_count\n",
    "    \n",
    "    def get_support(self, key):\n",
    "        key = self.__get_key(key)\n",
    "        if key not in self.counter.keys():\n",
    "            return 0\n",
    "        return self.counter[key]\n",
    "    \n",
    "    def __rule_mining(self):\n",
    "        keys = list(self.counter.keys())\n",
    "        for i in range(len(self.counter)-1):\n",
    "            for j in range(i+1, len(self.counter)):\n",
    "                a = self.__get_set(keys[i])\n",
    "                b = set.difference(self.__get_set(keys[j]), self.__get_set(keys[i]))\n",
    "                con = self.get_confidence(a, b)\n",
    "                if con >= self.min_confidence:\n",
    "                    self.rules.add((self.__get_key(a), self.__get_key(b), round(con, 4)))\n",
    "                con = self.get_confidence(b, a)\n",
    "                if con >= self.min_confidence:\n",
    "                    self.rules.add((self.__get_key(b), self.__get_key(a), round(con, 4)))\n",
    "\n",
    "    def fit(self):\n",
    "        if self.max_length is None:\n",
    "            self.__build_counter(len(self.chars))\n",
    "        else:\n",
    "            self.__build_counter(self.max_length)\n",
    "        self.__dict_merger()\n",
    "        self.__rule_mining()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'A', 'B', 'E'},\n",
       " {'B', 'D'},\n",
       " {'B', 'C'},\n",
       " {'A', 'B', 'D'},\n",
       " {'A', 'C'},\n",
       " {'B', 'C'},\n",
       " {'A', 'C'},\n",
       " {'A', 'B', 'C', 'E'},\n",
       " {'A', 'B', 'C'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_sample = [set(list(x)) for x in [\"ABE\", \"BD\", \"BC\", \"ABD\", \"AC\", \"BC\", \"AC\", \"ABCE\", \"ABC\"]]\n",
    "s_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = NaiveApriori(s_sample, 2, 0.60)\n",
    "obj.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'E': 2,\n",
       " 'B': 7,\n",
       " 'C': 6,\n",
       " 'D': 2,\n",
       " 'A': 6,\n",
       " 'B-D': 2,\n",
       " 'B-E': 2,\n",
       " 'B-C': 4,\n",
       " 'A-B': 4,\n",
       " 'A-E': 2,\n",
       " 'A-C': 4,\n",
       " 'A-B-E': 2,\n",
       " 'A-B-C': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('A', 'B', 0.6667),\n",
       " ('A', 'C', 0.6667),\n",
       " ('A-E', 'B', 1.0),\n",
       " ('B-E', 'A', 1.0),\n",
       " ('C', 'A', 0.6667),\n",
       " ('C', 'B', 0.6667),\n",
       " ('D', 'B', 1.0),\n",
       " ('E', 'A', 1.0),\n",
       " ('E', 'A-B', 1.0),\n",
       " ('E', 'B', 1.0)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {a, b, c} refers to rule: \"a implies b with confidence 100*c %\"\n",
    "\n",
    "obj.rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grocery Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = NaiveApriori(samples, 10000, 0.70, 1)\n",
    "obj.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.counter.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.rules"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
